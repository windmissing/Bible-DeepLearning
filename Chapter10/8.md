> **[warning]**  
> 这一节一点都看不懂  

从$h^{(t-1)}$到$h^{(t)}$的循环权重映射以及从$x^{(t)}$到$h^{(t)}$的输入权重映射是循环网络中最难学习的参数。
研究者{cite?}提出避免这种困难的方法是设定循环隐藏单元，使其能很好地捕捉过去输入历史，并且\emph{只学习输出权重}。  
> **[success]**  
> 设定h->h的参数  
> 学习x->o的参数  

回声状态网络或ESN{cite?}，以及流体状态机{cite?}分别独立地提出了这种想法。  
> **[warning]**  
> 回声状态网络?流体状态机?  

后者是类似的，只不过它使用脉冲神经元（二值输出）而不是ESN中的连续隐藏单元。  
> **[warning]**  
> 脉冲神经元?连续隐藏单元?
 
ESN和流体状态机都被称为储层计算{cite?}，因为隐藏单元形成了可能捕获输入历史不同方面的临时特征池。
> **[warning]**  
> 储层计算?输入历史不同方面的临时特征池?

储层计算循环网络类似于核机器，这是思考它们的一种方式：它们将任意长度的序列（到时刻$t$的输入历史）映射为一个长度固定的向量（循环状态$h^{(t)}$），之后可以施加一个线性预测算子（通常是一个线性回归）以解决感兴趣的问题。
训练准则就可以很容易地设计为输出权重的凸函数。
例如，如果输出是从隐藏单元到输出目标的线性回归，训练准则就是均方误差，由于是凸的就可以用简单的学习算法可靠地解决{cite?}。

因此，重要的问题是：我们如何设置输入和循环权重才能让一组丰富的历史可以在循环神经网络的状态中表示？
储层计算研究给出的答案是将循环网络视为动态系统，并设定让动态系统接近稳定边缘的输入和循环权重。

最初的想法是使状态到状态转换函数的Jacobian矩阵的特征值接近1。
如\sec?解释，循环网络的一个重要特征就是Jacobian矩阵的特征值谱$J^{(t)} = \frac{\partial s^{(t)}}{\partial s^{(t-1)}}$。
特别重要的是$J^{(t)}$的谱半径，定义为特征值的最大绝对值。

为了解谱半径的影响，可以考虑反向传播中Jacobian矩阵$J$不随$t$改变的简单情况。
例如当网络是纯线性时，会发生这种情况。
假设$J$特征值$\lambda$对应的特征向量为$v$。
考虑当我们通过时间向后传播梯度向量时会发生什么。
如果刚开始的梯度向量为$g$，然后经过反向传播的一个步骤后，我们将得到$J g$，$n$步之后我们会得到$J^n g$。
现在考虑如果我们向后传播扰动版本的$g$会发生什么。
如果我们刚开始是$g + \delta v$，一步之后，我们会得到$J(g + \delta v)$。
$n$步之后，我们将得到$J^n(g + \delta v)$。
由此我们可以看出，由$g$开始的反向传播和由$g+\delta v$开始的反向传播，$n$步之后偏离$\delta J^n v$。
如果$v$选择为$J$特征值$\lambda$对应的一个单位特征向量，那么在每一步乘Jacobian矩阵只是简单地缩放。
反向传播的两次执行分离的距离为$\delta | \lambda |^n$。
当$v$对应于最大特征值$|\lambda|$，初始扰动为$\delta$时这个扰动达到可能的最宽分离。

当$ | \lambda | > 1$，偏差$\delta | \lambda |^n$就会指数增长。
当$ | \lambda | < 1$，偏差就会变得指数小。

当然，这个例子假定Jacobian矩阵在每个时间步是相同的，即对应于没有非线性的循环网络。
当非线性存在时，非线性的导数将在许多时间步后接近零，并有助于防止因过大的谱半径而导致的爆炸。
事实上，关于回声状态网络的最近工作提倡使用远大于1的谱半径{cite?}。

我们上面说的这些关于用重复矩阵相乘进行反向传播的循环网络的事实同样适用于没有非线性的正向传播的网络，其状态为$h^{(t+1)} = h^{(t)\top} W $。

如果线性映射$W^\top$在$L^2$范数的测度下总是缩小$h$，那么我们说这个映射是收缩的。
当谱半径小于一，则从$h^{(t)}$到$h^{(t+1)}$的映射是收缩的，因此小变化在每个时间步后变得更小。
当我们使用有限精度（如32位整数）来存储状态向量时，必然会使得网络忘掉过去的信息。

Jacobian矩阵告诉我们$h^{(t)}$一个微小的变化如何向前一步传播，或等价的，$h^{(t+1)}$的梯度如何向后一步传播。
需要注意的是，$W$和$J$都不需要是对称的（尽管它们是实方阵），因此它们可能有复的特征值和特征向量，其中虚数分量对应于潜在的振荡行为（如果迭代地应用同一Jacobian）。
即使$h^{(t)}$或$h^{(t)}$中有趣的小变化在反向传播中是实值的，它们仍可以用这样的复数基表示。
重要的是，当向量乘以矩阵时，这些复数基的系数幅值（复数的绝对值）会发生什么变化。
幅值大于1的特征值对应于放大（如果反复应用则指数增长）或收缩（如果反复应用则指数减小）。

非线性映射情况时，Jacobian会在每一步任意变化。
因此，动态量变得更加复杂。
然而，一个小的初始变化多步之后仍然会变成一个大的变化。
纯线性和非线性情况的一个不同之处在于使用压缩非线性（如$\tanh$）可以使循环动态量有界。
注意，即使前向传播动态量有界，反向传播的动态量仍然可能无界，例如，当$\tanh$序列都在它们状态中间的线性部分，并且由谱半径大于1的权重矩阵连接。
然而，所有$\tanh$单元同时位于它们的线性激活点是非常罕见的。

回声状态网络的策略是简单地固定权重使其具有一定的谱半径如3，其中信息通过时间前向传播，但会由于饱和非线性单元（如$\tanh$）的稳定作用而不会爆炸。

最近，已经有研究表明，用于设置ESN权重的技术可以用来\emph{初始化}完全可训练的循环网络的权重（通过时间反向传播来训练隐藏到隐藏的循环权重），帮助学习长期依赖{cite?}。
在这种设定下，结合\sec?中稀疏初始化的方案，设置$1.2$的初始谱半径表现不错。