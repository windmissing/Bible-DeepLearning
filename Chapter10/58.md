# basic function
不管sequence有多长，都会反复不断使用的function  
6'05''

# 由RNN产生句子

句子由字符/单词组成，称为一个token  
RNN每次产生一个token  
x: 上一次产生的token  
y: 预测的所有token的分布，需要使用sample或argmax从分布中选一个token  
## 使用
58'31''
y1 = p(W|<BOS>)  <BOS> = begin of sentence  
y2 = p(W|<BOS>, sample(y1))
y3 = p(W|<BOS>, sample(y1), sample(y2))
直至生成token<EOS>

问：为什么公式y2中仍以<BOS>为条件？  
答：<BOS>通过路径h'影响y2  
## 训练
输入x1,<BOS>，计算corss-entropy(y1, 目标1)
输入x2，计算corss-entropy(y2, 目标2)
。。。
总代价为所有单个单价之和。  
59'50''

# 用RNN产生 图片  
把图片看成一个句子
每个pixel是一个token  
1:03'21''

# 条件序列生成 conditional sequence generation

## Image Caption Generation

输入一张图片，输出一句话  
图-> CNN -> 向量  
这个向量所有RNN每个时刻的输入。  
所以时刻都输出这同一个向量。  
t0时刻再输入一个<BOS>  
1:06'59''

## 以字符序列为条件  
例如：翻译，chat-bot  
先用一个向量包含整个句子的信息（RNN encoding）  
把向量作为每个tt的输入(RNN docoder)  
encoder和decoder是一起train的  
又叫seq2seq learning

# 动态条件序列生成 Dynamic Conditional Generation

1:15'09''
问：机器怎么从h中节选出h1、h2并得到C1?  
答：Attention Based Model，1:18'27'',1:19'59'',1:24'24''  

# Tips for Generation

## Mismatch between Train and Test  
Training 1:38'09'', Testing 1:39'45''  
这个不一致会有什么问题？  
假设所有可能的路径为图中的树。  
机器不可能把所有的路径都学一遍，只学了这个树的子集。  
在test的时候，一步错=》进入到了未学习的子集-》步步错 1:43'02''
解决方法：  
（1）让training和testing的process match 1:43'33''  
后面的步骤以自己前面的步骤的输出为输入 1:44'39''  
而自己前的输出在学习过程中会变化 1:45'20''  
一但前面的输出变化了，此前基于过去的输出的训练都白训练了  
In practice, it's hard to train in this way.  
（2）scheduld sampling  
随机选择“正确答案”和“t-1的输出”  
一开始让“正确答案”的几率最高  
随后“正确答案”几率下降，“t-1的输出”几率上升  
1:47'05''  
（3）Beam Search  
每个t的输出是一个分布，接下来根据分布选择一个结果。  
如果每次都选argmax，选出来的路径不一定是argmax。 1:49'18''  
解决方法：keep several best pathes at each step 1:50'37''
问：RNN会产生所有符号的概率分布o，从o中sample一个符号作为输出y，为什么要把y作为下一次的输入而不是o?  
答：没听懂