**循环神经网络**(recurrent neural network)或RNN是一类用于处理序列数据的神经网络。
就像卷积网络是专门用于处理网格化数据$X$（如一个图像）的神经网络，循环神经网络是专门用于处理序列$x^{(1)}, \dots, x^{(\tau)}$的神经网络。
正如卷积网络可以很容易地扩展到具有很大宽度和高度的图像，以及处理大小可变的图像，循环网络可以扩展到更长的序列（比不基于序列的特化网络长得多）。
大多数循环网络也能处理可变长度的序列。 

> **[success]**  
> CNN的局限性：
某个输入对应的输出，不止取决于输入本身，还取决于之前的输入。  
例如：  
“arrive Taipei in November 2nd”中的Taipei应该分类为“destination”。  
“leave Taipei in November 2nd”中的Taipei应该分类为“departure”。  
单纯针对当前输入做训练难以得到好的效果。  
解决方法：把上一次的信息也作为这一次的输入。  

从多层网络出发到循环网络，我们需要利用上世纪80年代机器学习和统计模型早期思想的优点：**在模型的不同部分共享参数**。
参数共享使得模型能够扩展到不同形式的样本（这里指不同长度的样本）并进行泛化。
如果我们在每个时间点都有一个单独的参数，我们不但不能泛化到训练时没有见过序列长度，也不能在不同序列长度和不同时间位置上共享统计强度。
当信息的特定部分会在序列内多个位置出现时，这样的共享尤为重要。
例如，考虑这两句话："I went to Nepal in 2009"和"In 2009, I went to Nepal." 
如果我们让一个机器学习模型读取这两个句子，并提取叙述者去Nepal的年份，无论"2009年"是作为句子的第六个单词还是第二个单词出现，我们都希望模型能认出"2009年"为相关资料片段。
假设我们要训练一个处理固定长度句子的前馈网络。
传统的全连接前馈网络会给每个输入特征分配一个单独的参数，所以需要分别学习句子每个位置的所有语言规则。
相比之下，循环神经网络在几个时间步内共享相同的权重，不需要分别学习句子每个位置的所有语言规则。

一个相关的想法是在1维时间序列上使用卷积。
这种卷积方法是时延神经网络的基础。
卷积操作允许网络跨时间共享参数，但这种共享是浅层的。  
> **[success]** 问为什么说CNN卷积中的参数共享是浅层的？  
答：因为CNN卷积中的参数共享仅仅发一生卷积这一层。另个卷积层则共享另一组参数。  
而RNN中的参数共享发生在每个时间步。即在所有时间步中都共享同一组参数。  
从影响范围来讲，认为CNN卷积中的参数共享是浅层的，而RNN中的参数共享是深层的。  

卷积的输出是一个序列，其中输出中的每一项是相邻几项输入的函数。
参数共享的概念体现在每个时间步中使用的相同卷积核。
循环神经网络以不同的方式共享参数。
**输出的每一项是前一项的函数。
输出的每一项通过对先前的输出应用相同的更新规则而产生。**
这种循环方式导致参数通过很深的计算图共享。

为简单起见，我们说的RNN是指在序列上的操作，并且该序列在时刻$t$（从1到$\tau$）包含向量$x^{(t)}$。
在实际情况中，**循环网络通常在序列的小批量上操作，并且小批量的每项具有不同序列长度$\tau$**。
我们省略了小批量索引来简化记号。
此外，时间步索引不必是字面上现实世界中流逝的时间。
有时，它仅表示序列中的位置。
RNN也可以应用于二维的空间数据（如图像）。
甚至当应用于涉及时间的数据，该网络也可具有时间上朝向过去方向的连接，前提是整个序列在提供给网络之前就已经被完整地观察到。  
> **[warning]** 后面的数据只使用前面的数据，为什么前提是完整的序列？  

本章将计算图的思想扩展到包括循环。
这些周期代表变量自身的值在未来某一时间步对自身值的影响。
这样的计算图允许我们定义循环神经网络。
然后，我们描述许多构建、训练和使用循环神经网络的不同方式。

本章将简要介绍循环神经网络，为获取更多详细信息，我们建议读者参考{Graves-book2012}的著作。