让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练。
当然，在实践中，我们拥有的数据量是很有限的。
解决这个问题的一种方法是创建假数据并添加到训练集中。
对于一些机器学习任务，创建新的假数据相当简单。

**对分类来说这种方法是最简单的**。
分类器需要一个复杂的高维输入$x$，并用单个类别标识$y$概括$x$。
这意味着分类面临的一个主要任务是要对各种各样的变换保持不变。
我们可以轻易通过转换训练集中的$x$来生成新的$(x, y)$对。

**这种方法对于其他许多任务来说并不那么容易**。
例如，除非我们已经解决了密度估计问题，否则在密度估计任务中生成新的假数据是很困难的。

数据集增强对一个具体的分类问题来说是特别有效的方法：对象识别。  
> **[success]**  
平移、旋转、缩放  

图像是高维的并包括各种巨大的变化因素，其中有许多可以轻易地模拟。
即使模型已使用卷积和池化技术（\chapref{chap:convolutional_networks}）对部分平移保持不变，沿训练图像每个方向平移几个像素的操作通常可以大大改善泛化。
许多其他操作如旋转图像或缩放图像也已被证明非常有效。

我们必须要小心，不能使用会改变类别的转换。
例如，光学字符识别任务需要认识到``b''和``d''以及``6''和``9''的区别，所以对这些任务来说，水平翻转和旋转$180^{\circ}$并不是合适的数据集增强方式。

能保持我们希望的分类不变，但不容易执行的转换也是存在的。
例如，平面外绕轴转动难以通过简单的几何运算在输入像素上实现。

数据集增强对语音识别任务也是有效的\citep{Jaitly_VTLP_2013}。  
> **[success]**  
注入噪声、添加高斯噪声、随机地剪裁图像  

在神经网络的输入层注入噪声\citep{SietsmaDow91}也可以被看作是数据增强的一种方式。
对于许多分类甚至一些回归任务而言，即使小的随机噪声被加到输入，任务仍应该是能够被解决的。
然而 ，神经网络被证明对噪声不是非常健壮\citep{TangElias10}。
改善神经网络健壮性的方法之一是简单地将随机噪声添加到输入再进行训练。
输入噪声注入是一些无监督学习算法的一部分，如去噪自编码器\citep{VincentPLarochelleH2008}。
向隐藏单元施加噪声也是可行的，这可以被看作在多个抽象层上进行的数据集增强。
\cite{Poole14}最近表明，噪声的幅度被细心调整后，该方法是非常高效的。
我们将在\secref{sec:dropout}介绍一个强大的正则化策略Dropout，该策略可以被看作是通过与噪声\emph{相乘}构建新输入的过程。

在比较机器学习基准测试的结果时，考虑其采取的数据集增强是很重要的。
通常情况下，**人工设计的数据集增强方案可以大大减少机器学习技术的泛化误差**。
将一个机器学习算法的性能与另一个进行对比时，对照实验是必要的。
在比较机器学习算法A和机器学习算法B时，应该确保这两个算法使用同一人工设计的数据集增强方案。
假设算法$A$在没有数据集增强时表现不佳，而$B$结合大量人工转换的数据后表现良好。
在这样的情况下，很可能是合成转化引起了性能改进，而不是机器学习算法$B$比算法$A$更好。 
有时候，确定实验是否已经适当控制需要主观判断。
例如，向输入注入噪声的机器学习算法是执行数据集增强的一种形式。
通常，普适操作（例如，向输入添加高斯噪声）被认为是机器学习算法的一部分，
而特定于一个应用领域（如随机地裁剪图像）的操作被认为是独立的预处理步骤。
