手动设置超参数，我们必须了解超参数、训练误差、泛化误差和计算资源（内存和运行时间）之间的关系。
这需要切实了解一个学习算法有效容量的基础概念，如\chap?所描述的。
<!-- % 416 mid -->


手动搜索超参数的目标通常是最小化受限于运行时间和内存预算的泛化误差。
我们不去探讨如何确定各种超参数对运行时间和内存的影响，因为这高度依赖于平台。


手动搜索超参数的主要目标是调整模型的有效容量以匹配任务的复杂性。
有效容量受限于三个因素：模型的表示容量、学习算法成功最小化训练模型代价函数的能力以及代价函数和训练过程正则化模型的程度。
具有更多网络层，每层有更多隐藏单元的模型具有较高的表示能力——能够表示更复杂的函数。
然而，如果训练算法不能找到某个合适的函数来最小化训练代价，或是正则化项（如权重衰减）排除了这些合适的函数，那么即使模型的表达能力较高，也不能学习出合适的函数。%??  还是太乱

<!-- % 416 mid -->


当泛化误差以某个超参数为变量，作为函数绘制出来时，通常会表现为U形曲线，如\fig?所示。
在某个极端情况下，超参数对应着低容量，并且泛化误差由于训练误差较大而很高。
这便是欠拟合的情况。
另一种极端情况，超参数对应着高容量，并且泛化误差由于训练误差和测试误差之间的差距较大而很高。
最优的模型容量位于曲线中间的某个位置，能够达到最低可能的泛化误差，由某个中等的泛化差距(generalization gap)和某个中等的训练误差相加构成。
<!-- % -- 416 end -->



对于某些超参数，当超参数数值太大时，会发生过拟合。
例如中间层隐藏单元的数量，增加数量能提高模型的容量，容易发生过拟合。%??  原文不符 
对于某些超参数，当超参数数值太小时，也会发生过拟合。
例如，最小的权重衰减系数允许为零，此时学习算法具有最大的有效容量，反而容易过拟合。%??  原文不符   这一段好像偏差较大 
<!-- % 417 head -->


并非每个超参数都能对应着完整的U形曲线。
很多超参数是离散的，如中间层单元数目或是~maxout单元中线性元件的数目，这种情况只能沿曲线探索一些点。
有些超参数是二值的。
通常这些超参数用来指定是否使用学习算法中的一些可选部分，如预处理步骤减去均值并除以标准差来标准化输入特征。
这些超参数只能探索曲线上的两点。
其他一些超参数可能会有最小值或最大值，限制其探索曲线的某些部分。%??    
例如，权重衰减系数最小是零。
这意味着，如果权重衰减系数为零时模型欠拟合，那么我们将无法通过修改权重衰减系数探索过拟合区域。
换言之，有些超参数只能减少模型容量。
<!-- % 417 mid -->


学习率可能是最重要的超参数。  
> **[success] Ng补充：超参数重要性排序**  
（1）学习率$\alpha$  
（2）Momentum中的$\rho$(0.9)、一个隐藏层的unit数、mini-batch size(2^6,2^7,2^8,2^9)    
（3）层数、学习率衰减率  
（4）Adam算法中的$\rho_1(0.9), \rho_2(0.999), \delta(1e-8)$  
**Ng补充：学习率衰减的一些方法**  
1 epoch = 1 pass through data  

$$
\begin{aligned}
\alpha = \frac{1}{1 + \text{DecayRate} * \text{epoch}} * \alpha_0    \\
\alpha = 0.95^{\text{epoch}} * \alpha_0    \\
\alpha = \frac{K}{\sqrt{\text{epoch}}} * \alpha_0    \\
\text{分段函数}
\end{aligned}
$$

如果你只有时间调整一个超参数，那就调整学习率。
相比其他超参数，它以一种更复杂的方式控制模型的有效容量——当学习率适合优化问题时，模型的有效容量最高，此时学习率是\emph{正确}的，既不是特别大也不是特别小。
学习率关于\emph{训练误差}具有U形曲线，如\fig?所示。
当学习率过大时，梯度下降可能会不经意地增加而非减少训练误差。
在理想化的二次情况下，如果学习率是最佳值的两倍大时，会发生这种情况{cite?}。
当学习率太小，训练不仅慢，还有可能永久停留在一个很高的训练误差。
关于这种效应，我们知之甚少（不会发生于一个凸损失函数中）。
<!-- % 417 end -->

{% raw %}

\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter11/figures/lr_color}}
\fi
\caption{训练误差和学习率之间的典型关系。
注意当学习率大于最优值时误差会有显著的提升。
此图针对固定的训练时间，越小的学习率有时候可以以一个正比于学习率减小量的因素来减慢训练过程。
泛化误差也会得到类似的曲线，或者由于学习率过大或过小引起的正则化作用而变得复杂化。
由于一个糟糕的优化从某种程度上说可以避免过拟合，即使是训练误差相同的点也会拥有完全不同的泛化误差。}
\end{figure}
{% endraw %}
<!-- % 417 end -->
调整学习率外的其他参数时，需要同时监测训练误差和测试误差，以判断模型是否过拟合或欠拟合，然后适当调整其容量。

如果训练集错误率大于目标错误率，那么只能增加模型容量以改进模型。   
> **[success]**  
> 训练集错误率大于目标错误率 = 欠拟合 = 高偏差  
> 解决方法：（1）增加网络容量（2）训练更长时间（3）探索新的网络结构  

如果没有使用正则化，并且确信优化算法正确运行，那么有必要添加更多的网络层或隐藏单元。
然而，令人遗憾的是，这增加了模型的计算代价。

如果测试集错误率大于目标错误率，那么可以采取两个方法。  
> **[success]**  
> 测试集错误率大于目标错误率 = 过拟合 = 高方差  
> 解决方法：（1）更多的数据（2）正则化（3）探索新的网络结构   

测试误差是训练误差和测试误差之间差距与训练误差的总和。  
> **[danger]**  
> 测试误差与训练误差的定义不同。  

寻找最佳的测试误差需要权衡这些数值。  
> **[success]方差、偏差权衡**  
> 在早期的ML理论中，很难只调其中一种而不影响另一种。  
> 在DL学习中，只要网络足够大又正则化适当就能只减少偏差。只要数据足够多就能只减小方差。  

当训练误差较小（因此容量较大），测试误差主要取决于训练误差和测试误差之间的差距时，通常神经网络效果最好。
此时目标是缩小这一差距，使训练误差的增长速率不快于差距减小的速率。
要减少这个差距，我们可以改变正则化超参数，以减少有效的模型容量，如添加~Dropout~或权重衰减策略。
通常，最佳性能来自正则化得很好的大规模模型，比如使用~Dropout~的神经网络。
<!-- % 418 mid -->


大部分超参数可以通过推理其是否增加或减少模型容量来设置。
部分示例如表\?所示。


<!-- % 418 end -->
手动调整超参数时，不要忘记最终目标：提升测试集性能。
加入正则化只是实现这个目标的一种方法。
只要训练误差低，随时都可以通过收集更多的训练数据来减少泛化误差。
实践中能够确保学习有效的暴力方法就是不断提高模型容量和训练集的大小，直到解决问题。
这种做法增加了训练和推断的计算代价，所以只有在拥有足够资源时才是可行的。
原则上，这种做法可能会因为优化难度提高而失败，但对于许多问题而言，优化似乎并没有成为一个显著的障碍，当然，前提是选择了合适的模型。
<!-- % 419 end -->


| 超参数 | 容量何时增加 | 原因  | 注意事项 |
|---|---|---|---|
|隐藏单元数量 |  增加   | 增加隐藏单元数量会增加模型的表示能力。 | 几乎模型每个操作所需的时间和内存代价都会随隐藏单元数量的增加而增加。|
| 学习率 | 调至最优 | 不正确的学习速率，不管是太高还是太低都会由于优化失败而导致低有效容量的模型。|
| 卷积核宽度 | 增加 | 增加卷积核宽度会增加模型的参数数量。| 较宽的卷积核导致较窄的输出尺寸，除非使用隐式零填充减少此影响，否则会降低模型容量。 <br> 较宽的卷积核需要更多的内存存储参数，并会增加运行时间，但较窄的输出会降低内存代价。|
| 隐式零填充 | 增加 | 在卷积之前隐式添加零能保持较大尺寸的表示。 | 大多数操作的时间和内存代价会增加。|
| 权重衰减系数 | 降低 | 降低权重衰减系数使得模型参数可以自由地变大。 |
| Dropout 比率 | 降低 | 较少地丢弃单元可以更多地让单元彼此"协力"来适应训练集。 |