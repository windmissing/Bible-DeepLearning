> **[success]**  
DL的训练成本与ML高，因此常用随机搜索代替网格搜索  
搜索过程由粗粒度到细粒度。  
先粗粒度地随机搜索，找到一个最好的点，然后在这个点的附近进行细粒度地搜索。  
![](/assets/Chapter11/1.png)  


幸运的是，有一个替代网格搜索的方法，并且编程简单，使用更方便，能更快地收敛到超参数的良好取值：随机搜索~{cite?}。
<!-- % 422 -->


随机搜索过程如下。
首先，我们为每个超参数定义一个边缘分布，例如，Bernoulli分布或范畴分布（分别对应着二元超参数或离散超参数），或者对数尺度上的均匀分布（对应着正实值超参数）。
例如，
$$
\begin{aligned}
	\texttt{log\_learning\_rate} &\sim u(-1, -5), \\
	\texttt{learning\_rate} &= 10^{\texttt{log\_learning\_rate}},
\end{aligned}
$$

其中，$u(a,b)$表示区间$(a,b)$上均匀采样的样本。
类似地，$\texttt{log\_number\_of\_hidden\_units}$可以从$u(\log(50), \log(2000))$上采样。
<!-- % 422 mid -->


与网格搜索不同，我们\emph{不需要离散化}超参数的值。这允许我们在一个更大的集合上进行搜索，而不产生额外的计算代价。%?? bin 不知道如何翻译bin，你有什么好的翻译没 
实际上，如\fig?所示，当有几个超参数对性能度量没有显著影响时，随机搜索相比于网格搜索指数级地高效。
{Bergstra+Bengio-2012-small}进行了详细的研究并发现相比于网格搜索， 随机搜索能够更快地减小验证集误差（就每个模型运行的试验数而言）。
<!-- % 422 end -->

与网格搜索一样，我们通常会重复运行不同版本的随机搜索，以基于前一次运行的结果改进下一次搜索。
<!-- % 422 end -->


随机搜索能比网格搜索更快地找到良好超参数的原因是，没有浪费的实验，不像网格搜索有时会对一个超参数的两个不同值（给定其他超参数值不变）给出相同结果。
在网格搜索中，其他超参数将在这两次实验中拥有相同的值，而在随机搜索中，它们通常会具有不同的值。
因此，如果这两个值的变化所对应的验证集误差没有明显区别的话，网格搜索会没有必要地重复两个等价的实验，而随机搜索仍然会对其他超参数进行两次独立地探索。 

> **[success] Ng补充：搜索尺度问题**  
对于有些参数，在搜索范围内随机搜索参数值来测试。  
对于另一些参数，不能直接在搜索范围为随机搜索。  
例如学习率a，假设其范围是[1e-4, 1]，应该这样搜索：  
r = -4 * np.random.rand()  
a = 10^r  
而对于指数衰减平均的参数$\rho$，假设其范围是[0.9, 0.999]，应该这样搜索：  
r = np.random.rand(-3, -1)    
$\rho = 1 - 10^r$
这是因为a和$\rho$在不同位置灵敏度不同。  