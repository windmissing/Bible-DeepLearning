# Table of contents

* [Introduction](README.md)
* 第5章 机器学习基础
    * [5.1.2 性能度量P](Chapter5/1LearningAlgorithms/2PerformanceMeasure.md)
    * [5.2.1 没有免费午餐定理](Chapter5/2/1.md)
* [第6章 深度前馈网络](Chapter6/0Introduction.md)
    * [6.1 例子：学习XOR](Chapter6/1Examples.md)
    * [6.2 基于梯度的学习](Chapter6/2Gradient/0Introduction.md)
        * [6.2.1 代价函数](Chapter6/2Gradient/1Cost/0Introduction.md)
            * [6.2.1.1 使用最大似然学习条件分布](Chapter6/2Gradient/1Cost/1Likelihood.md)
            * [6.2.1.2 学习条件统计量](Chapter6/2Gradient/1Cost/2.md)
        * [6.2.2 输出单元](Chapter6/2Gradient/2OutputUnit/0Introduction.md)
            * [6.2.2.1 用于高斯输出分布的线性神单元](Chapter6/2Gradient/2OutputUnit/1Linear.md)
            * [6.2.2.2 用于Bernoulli输出分布的sigmoid单元](Chapter6/2Gradient/2OutputUnit/2Sigmoid.md)
            * [6.2.2.3 用于Multinoulli输出分布的softmax单元](Chapter6/2Gradient/2OutputUnit/3Softmax.md)
    * [6.3 隐藏单元](Chapter6/3Hidden/0Introduction.md)
        * [6.3.1 ReLU及其扩展](Chapter6/3Hidden/1ReLU.md)
        * [6.3.2 logistic sigmoid与双曲正切函数](Chapter6/3Hidden/2SigmoidTanh.md)
        * [6.3.3 其他隐藏单元](Chapter6/3Hidden/3Other.md)
        * [李宏毅补充 SELU](Chapter6/SELU.md)
    * [6.4 架构设计](Chapter6/4Architecture.md)
    * [6.5 反向传播和其他的微分算法](Chapter6/5Backprop/0Introduction.md)
        * [6.5.1 计算图](Chapter6/5Backprop/1ComputationalGraphs.md)
        * [6.5.2 微积分中的链式法则](Chapter6/5Backprop/2ChainRule.md)
        * [6.5.3 递归地使用链式法则来实现反向传播](Chapter6/5Backprop/3Recursively.md)
        * [6.5.4 全连接MLP中的反向传播计算](Chapter6/5Backprop/4FullyConnectedMLP.md)
        * [6.5.5 符号到符号的导数](Chapter6/5Backprop/5Diveriation.md)
        * [6.5.6 一般化的反向传播](Chapter6/5Backprop/6-5-6.md)
        * [6.5.7 实例：用于MLP 训练的反向传播](Chapter6/5Backprop/7MLPTraining.md)
        * [6.5.8 复杂化](Chapter6/5Backprop/8Complications.md)
* [第7章 深度学习中的正则化](Chapter7/0Introduction.md)
    * [7.1 参数范数惩罚](Chapter7/1ParameterNormPenalties/0Introduction.md)
        * [7.1.1 L2参数正则化](Chapter7/1ParameterNormPenalties/1L2.md)
        * [7.1.2 L1参数正则化](Chapter7/1ParameterNormPenalties/2L1.md)
    * [7.2 作为约束的范数惩罚](Chapter7/2ConstrainedOptimization.md)
    * [7.3 正则化和欠约束问题](Chapter7/3UnderConstrainedProblems.md)
    * [7.4 数据集增强](Chapter7/4DatasetAugmentation.md)
    * [7.5 噪声鲁棒性](Chapter7/5NoiseRobustness.md)
    * [7.6 半监督学习](Chapter7/6SemiSupervised.md)
    * [7.7 多任务学习](Chapter7/7Multitask.md)
    * [7.8 提前终止](Chapter7/8EarlyStopping.md)
    * [7.9 参数绑定和参数共享](Chapter7/9ParameterSharing.md)
    * [7.10 稀疏表示](Chapter7/10SparseRepresentations.md)
    * [7.11 Bagging 和其他集成方法](Chapter7/11Bagging.md)
    * [7.12 Dropout](Chapter7/12Dropout.md)
    * [7.13 对抗训练](Chapter7/13AdversarialTraining.md)
    * [7.14 切面距离、正切传播和流形正切分类器](Chapter7/14Tangent.md)
    * [Ag补充 一些能用于提升比赛成绩的方法](Chapter7/Contest.md)
    * [李宏毅补充 Domain-Adversarial Training](Chapter7/DomainAdversarial.md)
    * [李宏毅补充 Zero-Shot Training](Chapter7/ZeroShot.md)
    * [李宏毅补充 深度自编码器](Chapter7/AutoEncoder.md)
* [第8章 深度模型中的优化](Chapter8/0Optimization.md)
    * [8.1 学习和纯优化有什么不同](Chapter8/1Difference/0Difference.md)
        * [8.1.1 经验风险最小化](Chapter8/1Difference/1EmpiricalRiskMinimization.md)
        * [8.1.2 代理损失函数和提前终止](Chapter8/1Difference/2SurrogateLossFunctions.md)
        * [8.1.3 批量算法和小批量算法](Chapter8/1Difference/3Minibatch.md)
    * 8.2 神经网络优化中的挑战
        * [8.2.1 病态](Chapter8/2Challenges/1IllConditioning.md)
        * [8.2.2 局部极小值](Chapter8/2Challenges/2LocalMinima.md)
        * [8.2.3 8.2.3 高原、鞍点和其他平坦区域](Chapter8/2Challenges/3Saddle.md)
        * [8.2.4 悬崖和梯度爆炸](Chapter8/2Challenges/4ExplodingGradients.md)
        * [8.2.5 长期依赖](Chapter8/2Challenges/5LongTermDependencies.md)
        * [8.2.6 非精确梯度](Chapter8/2Challenges/6InexactGradients.md)
    * 8.3 基本算法
        * [8.3.1 随机梯度下降](Chapter8/3BasicAlgorithms/1SGD.md)
        * [8.3.2 动量](Chapter8/3BasicAlgorithms/2Momentum.md)
        * [8.3.3 Nesterov 动量](Chapter8/3BasicAlgorithms/3Nesterov.md)
    * [8.4 参数初始化策略](Chapter8/4ParameterInitialization.md)
    * 8.5 自适应学习率算法
        * [8.5.1 AdaGrad](Chapter8/5AdaptiveLearningRates/1AdaGrad.md)
        * [8.5.2 RMSProp](Chapter8/5AdaptiveLearningRates/2RMSProp.md)
        * [8.5.3 Adam](Chapter8/5AdaptiveLearningRates/3Adam.md)
        * [8.5.4 选择正确的优化算法](Chapter8/5AdaptiveLearningRates/4Choose.md)
    * 8.6 二阶近似方法
        * [8.6.1 牛顿法](https://windmissing.github.io/mathematics_basic_for_ML/NumericalComputation/Newton.html)
        * [8.6.2 共轭梯度](Chapter8/6ApproximateSecondOrder/2Conjugate.md)
        * [8.6.3 BFGS](Chapter8/6ApproximateSecondOrder/3BFGS.md)
    * 8.7 优化策略和元算法
        * [8.7.1 批标准化](Chapter8/7Strategies/1BatchNormalization.md)
        * [8.7.2 坐标下降](Chapter8/7Strategies/2CoordinateDescent.md)
        * [8.7.3 Polyak 平均](Chapter8/7Strategies/3PolyakAveraging.md)
        * [8.7.4 监督预训练](Chapter8/7Strategies/4SupervisedPretraining.md)
        * [8.7.5 设计有助于优化的模型](Chapter8/7Strategies/5AidOptimization.md)
* [第9章 卷积网络](Chapter9/0cnn.md)
    * [9.1 卷积运算](Chapter9/1Convolution.md)
    * [9.2 动机](Chapter9/2Motivation.md)
    * [9.3 池化](Chapter9/3Pooling.md)
    * [9.4 卷积与池化作为一种无限强的先验](Chapter9/4Prior.md)
    * [9.5 基本卷积函数的变体](Chapter9/5Variant.md)
    * [9.6 结构化输出](Chapter9/6StructuredOutputs.md)
    * [9.7 数据类型](Chapter9/7Data.md)
    * [Ag补充 经典网络](Chapter9/ClassicNetwork.md)
    * [Ag补充 残差网络](Chapter9/ResNet.md)
    * [Ag补充 1*1卷积](Chapter9/SpecialConv.md)
    * [Ag补充 Inception网络](Chapter9/Inception.md)
    * [Ag补充  图像的定位分类问题](Chapter9/ClassificationWithLocalization.md)
    * [Ag补充 人脸识别问题](Chapter9/FaceDetection.md)
    * [Ag补充 风格迁移](Chapter9/Style.md)
    * [李宏毅补充 Highway Network](Chapter9/Highway.md)
* [第10章 序列建模：循环和递归网络](Chapter10/Introduction.md)
    * [10.1 展开计算图](Chapter10/1Unfolding.md)
    * [10.2 循环神经网络](Chapter10/2RNN/0RNN.md)
        * [10.2.1 导师驱动过程和输出循环网络](Chapter10/2RNN/1TeacherForcing.md)
        * [10.2.2 计算循环神经网络的梯度](Chapter10/2RNN/2BPTT.md)
        * [10.2.3 作为有向图模型的循环网络](Chapter10/2RNN/3.md)
        * [10.2.4 基于上下文的RNN序列建模](Chapter10/2RNN/4.md)
    * [10.3 双向RNN](Chapter10/3Bidirectional.md)
    * [10.4 基于编码 - 解码的序列到序列架构](Chapter10/4EncoderDecoder.md)
    * [10.5 深度循环网络](Chapter10/5Deep.md)
    * [10.6 递归神经网络](Chapter10/6.md)
    * [10.7 长期依赖的挑战](Chapter10/7LongDependency.md)
    * [10.9 渗漏单元和其他多时间尺度的策略](Chapter10/9.md)
    * [10.10 长短期记忆和其他门控RNN](Chapter10/10Gate/0.md)
        * [10.10.1 LSTM](Chapter10/10Gate/1LSTM.md)
        * [10.10.2 其他门控RNN](Chapter10/10Gate/2OtherGates.md)
    * 10.11 优化长期依赖
        * [10.11.1 梯度截断](Chapter10/11OptimizationLTD/1.md)
        * [10.11.2 引导信息流的正则化](Chapter10/11OptimizationLTD/2.md)
    * [10.12 外显记忆](Chapter10/12.md)
    * [李宏毅/Ag补充 RNN的各种应用](Chapter10/Applications.md)
    * [李宏毅/Ag补充 注意力模型](Chapter10/Attention.md)
    * [李宏毅补充 Deep Learing VS Structured Learning](Chapter10/Structured.md)
    * [Ag补充 语言模型](Chapter10/LanguageModel.md)
    * [Ag补充 词汇表征](Chapter10/WordRepresentation.md)
    * [Ag补充 类比推理](Chapter10/ReasonableAnalogies.md)
    * [Ag补充 情绪分类](Chapter10/SentimentClassification.md)
    * [Ag补充 Seq2Seq](Chapter10/Seq2Seq.md)
    * [Ag补充 语音辨识](Chapter10/Speech.md)
    * [Ag补充 触发字检测](Chapter10/TriggerWord.md)
    * [李宏毅补充 Embedding](Chapter10/Embedding.md)
    * [李宏毅补充 指针网络](Chapter10/PointerNetwork.md)
    * [李宏毅补充 循环网络](Chapter10/Recursive.md)
* [第11章 实践方法论](Chapter11/0PracticalMethodology.md)
    * [11.1 性能度量](Chapter11/1Performance.md)
    * [11.2 默认的基准模型](Chapter11/2Baseline.md)
    * [11.3 决定是否收集更多数据](Chapter11/3MoreData.md)
    * [11.4 选择超参数](Chapter11/4Hyperparameters/0Hyperparameters.md)
        * [11.4.1 手动选择超参数](Chapter11/4Hyperparameters/1Manual.md)
        * [11.4.3 网络搜索](Chapter11/4Hyperparameters/1Manual.md)
        * [11.4.4 随机搜索](Chapter11/4Hyperparameters/1Manual.md)
    * [11.5 调试策略](Chapter11/5Debug.md)  
* 李宏毅补充 GAN
    * [深度生成模型](GAN/Generative.md)
    * [条件生成对抗网络](GAN/Condition.md)
    * [非监督条件生成](GAN/unsupervised.md)
    * [GAN的理论基础](GAN/Thoery.md)
    * [通用框架](GAN/Framework.md)
    * [LSGAN](GAN/LSGAN.md)
    * [WGAN](GAN/WGAN.md)
    * [Energy-based GAN](GAN/EBGAN.md)
    * [Info GAN](GAN/Info.md)
    * [VAE GAN](GAN/VAEGAN.md)
    * [BiGAN](GAN/BiGAN.md)
    * [Domain Adversarial Training](GAN/DAT.md)
    * [照片编辑应用](GAN/Phote.md)
    * [序列生成的应用](GAN/Sequence.md)
    * [GAN的估计](GAN/Evaluation.md)
* 李宏毅补充 RL
    * [增强学习](RL/Reinforce.md)
    * Policy Based
        * [Basic Version](RL/Policy1.md)
        * [公式改进](RL/Policy2.md)
        * [off-policy](RL/Policy3.md)
        * [近端优化策略](RL/Policy4.md)
    * Value Based
        * [value function](RL/Value1.md)
        * [Q-Learning](RL/Value2.md)
        * [Q-Learning的改进算法](RL/Value3.md)
    * [Q-Learning Vs Policy Based](RL/Compare.md)
    * Q-Learning结合Policy Based
        * [复习](RL/A3C1.md)
        * [A3C](RL/A3C2.md)
        * [Pathwise Derivative Policy Gradient](RL/A3C3.md)
    * [稀疏奖励](RL/Sparse.md)
    * [模仿学习](RL/Imitation.md)
* 李宏毅补充 
    * [异常侦测](1209400866/Anomaly.md)
        * [case 1：labelled data](1209400866/Labelled.md)
        * [case 3：polluted unlabelled data](1209400866/Unlabelled.md)
    * 对抗模型 attack ML models
        * [攻击](1209400866/Attack.md)
        * [防御](1209400866/Defense.md)