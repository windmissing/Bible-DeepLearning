凸优化问题的一个突出特点是其可以简化为寻找一个局部极小点的问题。
任何一个局部极小点都是全局最小点。
有些凸函数的底部是一个平坦的区域，而不是单一的全局最小点，但该平坦区域中的任意点都是一个可以接受的解。
优化一个凸问题时，若发现了任何形式的临界点，我们都会知道已经找到了一个不错的可行解。

对于非凸函数时，如神经网络，有可能会存在多个局部极小值。
事实上，几乎所有的深度模型基本上都会有非常多的局部极小值。
然而，我们会发现这并不是主要问题。

# 模型可辨识性

由于**模型可辨识性**（model identifiability）问题，神经网络和任意具有多个等效参数化潜变量的模型都会具有多个局部极小值。
如果一个足够大的训练集可以唯一确定一组模型参数，那么该模型被称为可辨认的。  
> **[success]**  
模型的可辨识性：  
假设训练集足够大，最后只会得到一个唯一的模型。  
不会有两个效果等价的模型。  

# DL不是可辨识模型

带有潜变量的模型通常是不可辨认的，因为通过相互交换潜变量我们能得到等价的模型。  
> **[success]**    
原因一：DL具有权重空间对称性。即一个隐藏层中两个unit的权重交换。得到的模型的等价的。  

例如，考虑神经网络的第一层，我们可以交换单元$i$和单元$j$的传入权重向量、传出权重向量而得到等价的模型。
如果神经网络有$m$层，每层有$n$个单元，那么会有$n!^m$种排列隐藏单元的方式。
这种不可辨认性被称为**权重空间对称性**（weight space symmetry）。

除了权重空间对称性，很多神经网络还有其他导致不可辨认的原因。  
> **[success]**    
原因二：maxout中的权重的扩大与缩小。  

例如，在任意整流线性网络或者~maxout~网络中，
我们可以将传入权重和偏置放缩$\alpha$倍，然后将传出权重放缩$\frac{1}{\alpha}$倍，而保持模型等价。
这意味着，如果代价函数不包括如权重衰减这种直接依赖于权重而非模型输出的项，那么整流线性网络或者~maxout~网络的每一个局部极小点都在等价的局部极小值的$(m\times n)$维双曲线上。

# 不可辨识模型有多个局部极小值

这些模型可辨识性问题意味着神经网络代价函数具有非常多、甚至不可数无限多的局部极小值。
然而，所有这些由于不可辨识性问题而产生的局部极小值都有相同的代价函数值。
因此，这些局部极小值并非是非凸所带来的问题。
> **[success]**  
等价的两个模型，对相同的输入会得到相同的代价。  
如果一个模型的代价在极小值点，那么等价的另一个模型也在极小值点，且代价值相同。  

# 局部极小值的危害

**如果局部极小值相比全局最小点拥有很大的代价，局部极小值会带来很大的隐患**。
我们可以构建没有隐藏单元的小规模神经网络，其局部极小值的代价比全局最小点的代价大很多~{cite?}。
如果具有很大代价的局部极小值是常见的，那么这将给基于梯度的优化算法带来极大的问题。

对于实际中感兴趣的网络，是否存在大量代价很高的局部极小值，优化算法是否会碰到这些局部极小值，都是尚未解决的公开问题。
多年来，大多数从业者认为局部极小值是困扰神经网络优化的常见问题。
如今，情况有所变化。
这个问题仍然是学术界的热点问题，但是学者们现在猜想，对于足够大的神经网络而言，
大部分局部极小值都具有很小的代价函数，我们能不能找到真正的全局最小点并不重要，而是需要在参数空间中找到一个代价很小（但不是最小）的点~{cite?}。
> **[success]**  
实际上局部极小值并不常见，因此危害并没有那么大。  
如果在优化过程中遇到问题，应该仔细排查是不是真的由于局部极小值引起的。    
排除方法：画出梯度范数随时间的变化  

很多从业者将神经网络优化中的所有困难都归结于局部极小值。
我们鼓励从业者要仔细分析特定的问题。
一种能够排除局部极小值是主要问题的检测方法是画出梯度范数随时间的变化。
如果梯度范数没有缩小到一个微小的值，那么该问题既不是局部极小值，也不是其他形式的临界点。

在高维空间中，很难明确证明局部极小值是导致问题的原因。
许多并非局部极小值的结构也具有很小的梯度。  

> **[success] Ng补充**  
在高维空间中，局部最小值并不常见，更多的是鞍点。因此local optima不是主要的Challenge。而plateau危害更大。因为plateaus can make learning slow.  