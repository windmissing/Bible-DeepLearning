深度神经网络设计中的一个重要方面是代价函数的选择。
幸运的是，神经网络的代价函数或多或少是和其他的参数模型例如线性模型的代价函数相同的。

在大多数情况下， 我们的参数模型定义了一个分布$p(y\mid x;\theta)$并且我们简单地使用最大似然原理。
这意味着我们使用训练数据和模型预测间的交叉熵作为代价函数。

> **[success]**  
经常使用数据分布与模型分布的交叉熵作为代价函数。  
因此，模型分布的输出层决定了交叉熵的形式，也决定了代价函数的形式。  
6.2节讨论的这些单元可以用于输出层神经元，也可以用作中间层神经元。但这里只讨论它们作为输出层神经元时的情况。以及它们对应的代码函数  
*神经元=单元，不同的书上用的术语不同。*

有时，我们使用一个更简单的方法，不是预测$y$的完整概率分布，而是仅仅预测在给定$x$的条件下$y$的某种统计量。
某些专门的损失函数允许我们来训练这些估计量的预测器。  

> **[success]**  
不是预测$p(y;\theta)$的分布，而是预测$p(f(y)|x;\theta)$的分布

用于训练神经网络的完整的代价函数，通常在我们这里描述的基本代价函数的基础上结合一个正则项。
我们已经在\sec?中看到正则化应用到线性模型中的一些简单的例子。
用于线性模型的权重衰减方法也直接适用于深度神经网络，而且是最流行的正则化策略之一。  

> **[info]**  
[权重衰减](https://windmissing.github.io/Bible-DeepLearning/Chapter7/1ParameterNormPenalties/1L2.html)  

用于神经网络的更高级的正则化策略将在[第7章](https://windmissing.github.io/Bible-DeepLearning/Chapter7/0Introduction.html)中讨论。