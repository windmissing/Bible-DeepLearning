有时我们并不是想学习一个完整的概率分布$p(y\mid x; \theta)$，而仅仅是想学习在给定$x$时$y$的某个条件统计量。

> **[success]**  
学习$p(f(y)|x;\theta)$而不是$p(y;\theta)$

例如，我们可能有一个预测器$f(x; \theta)$，我们想用它来预测$y$的均值。
如果我们使用一个足够强大的神经网络，我们可以认为这个神经网络能够表示一大类函数中的任何一个函数$f$，这个类仅仅被一些特征所限制，例如连续性和有界，而不是具有特殊的参数形式。
从这个角度来看，我们可以把代价函数看作是一个泛函而不仅仅是一个函数。
泛函是函数到实数的映射。我们因此可以将学习看作是选择一个函数而不仅仅是选择一组参数。  

> **[success]**  
一个网络表示一个函数的集合。  
一个网络加上一组特定参数表示一个函数。   
代价函数是一个泛函，它是函数到实数的映射。在这里就是指（模型+参数）到y的均值的映射。  

我们可以设计代价泛函在我们想要的某些特殊函数处取得最小值。
例如，我们可以设计一个代价泛函，使它的最小值处于一个特殊的函数上，这个函数将$x$映射到给定$x$时$y$的期望值。  
> **[success]**  
神经网络定义了一个函数的集合。我们想要找的是其中最合适的函数。但不知道怎样是最合适的函数。    
假设有一种泛函，能够将函数的集合映射到标量的集合。  
通过合理的设计这个泛函，可以认为得到的标量集合中，符合某个特点的标量，其对应的函数是最合适的函数。  
在前面章节中，就是通过这种方法，设计合理的泛函（代价函数），找到符号特点的标量（最小值），其对应的函数（具有某组参数的函数）是最合适的函数。  
在这一节，将使用另一种方法，即“直接寻找最优的函数”的方法，这种方法叫作“变分法”。使用这种方法能得到与之前类似的结果。  

对函数求解优化问题需要用到**变分法**（calculus of variations）这个数学工具，我们将在[19.4.2](TODO)中讨论。  
> **[warning]** 变分法？后面的结论也看不懂。    

理解变分法对于理解本章的内容不是必要的。
目前，只需要知道变分法可以被用来导出下面的两个结果。

我们使用变分法导出的第一个结果是解优化问题  
$$
\begin{aligned}
f^* = \arg\min_f  \Bbb E_{x, y \sim  p_\text{data}} ||y-f(x)||^2
\end{aligned}
$$

得到
$$
\begin{aligned}
f^*(x) = \Bbb E_{y\sim p_\text{data}(y|x)} [y],
\end{aligned}
$$

要求这个函数处在我们要优化的类里。
换句话说，如果我们能够用无穷多的、来源于真实的数据生成分布的样本进行训练，最小化均方误差代价函数将得到一个函数，它可以用来对每个$x$的值预测出$y$的均值。

不同的代价函数给出不同的统计量。
第二个使用变分法得到的结果是
$$
\begin{aligned}
f^* = \arg\min_f \Bbb E_{x, y \sim  p_\text{data}} ||y - f(x)||_1
\end{aligned}
$$

将得到一个函数可以对每个$x$预测$y$取值的\emph{中位数}，只要这个函数在我们要优化的函数族里。
这个代价函数通常被称为平均绝对误差。

可惜的是，均方误差和平均绝对误差在使用基于梯度的优化方法时往往成效不佳。
一些饱和的输出单元当结合这些代价函数时会产生非常小的梯度。
这就是为什么交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的原因之一了，即使是在没必要估计整个$p(y\mid x)$分布时。  
> **[warning]**  
这有什么可比性呢？一个是用于分类问题的，一个是用于回归问题了。  